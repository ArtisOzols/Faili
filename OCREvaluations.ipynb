{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WER,RIL, MER, WIL\n",
    "https://www.researchgate.net/publication/221478089_From_WER_and_RIL_to_MER_and_WIL_improved_evaluation_measures_for_connected_speech_recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jitsi/jiwer\n",
    "import jiwer \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5 0.75 0.45454545454545453\n",
      "\n",
      "Word Error Rate 0.6\n",
      "Characters Error Rate 0.17647058823529413\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "ground_truth = \"hello world\"\n",
    "hypothesis = \"hello duck\"\n",
    "\n",
    "wer = jiwer.wer(ground_truth, hypothesis)\n",
    "mer = jiwer.mer(ground_truth, hypothesis)\n",
    "wil = jiwer.wil(ground_truth, hypothesis)\n",
    "cer = jiwer.cer(ground_truth, hypothesis)\n",
    "\n",
    "# faster, because `compute_measures` only needs to perform the heavy lifting once:\n",
    "measures = jiwer.compute_measures(ground_truth, hypothesis)\n",
    "wer = measures['wer']\n",
    "mer = measures['mer']\n",
    "wil = measures['wil']\n",
    "# cer = measures['cer'] # cer is not included in jiwer.compute_measures :(\n",
    "print(wer, mer, wil, cer)\n",
    "print()\n",
    "\n",
    "#Another example\n",
    "two_sentence_truth = [\"i can spell\", \"i hope\"]\n",
    "two_sentence_hypothesis = [\"i kan cpell\", \"i hop\"]\n",
    "wer = jiwer.wer(two_sentence_truth, two_sentence_hypothesis)\n",
    "cer = jiwer.cer(two_sentence_truth, two_sentence_hypothesis)\n",
    "print(\"Word Error Rate\", wer)  # so 0.6 would mean 60% of words were wrong\n",
    "print(\"Characters Error Rate\", cer) # so 17% of all characters were wrong,missing or duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('txt/Slovo/slov1941n1_004.txt'),\n",
       " WindowsPath('txt/Slovo/slov1941n2_004.txt'),\n",
       " WindowsPath('txt/Slovo/slov1941n3_004.txt'),\n",
       " WindowsPath('txt/Slovo/slov1941n4_003.txt'),\n",
       " WindowsPath('txt/Slovo/slov1941n4_004.txt')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are adding files in \"slovo\" variable using \"pathlib\" library\n",
    "# Then we sort the list by file name\n",
    "slovo = [f for f in Path(\"./txt/Slovo\").glob(\"*.txt\")]\n",
    "slovo = sorted(slovo, key=lambda elem: elem.name)\n",
    "slovo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('txt/Slovo..._clean/slov1941n1_004_clean.txt'),\n",
       " WindowsPath('txt/Slovo..._clean/slov1941n2_004_clean.txt'),\n",
       " WindowsPath('txt/Slovo..._clean/slov1941n3_003_clean.txt'),\n",
       " WindowsPath('txt/Slovo..._clean/slov1941n4_003_clean.txt'),\n",
       " WindowsPath('txt/Slovo..._clean/slov1941n4_004_clean.txt')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slovo_clean  = [f for f in Path(\"./txt/Slovo..._clean\").glob(\"*.txt\")]\n",
    "slovo_clean = sorted(slovo_clean, key=lambda elem: elem.name)\n",
    "slovo_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2496"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we concatinate all pages together (because we need to get the general idea on how well OCR works about the journal not pages)\n",
    "# And we split text by lines\n",
    "s_text = \"\"\n",
    "for f in slovo:\n",
    "    s_text += f.read_text(encoding=\"utf-8\")+\"\\n\"\n",
    "s_lines = s_text.split(\"\\n\")[:-1]\n",
    "len(s_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2482"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_clean_text = \"\"\n",
    "for f in slovo_clean:\n",
    "    s_clean_text += f.read_text(encoding=\"utf-8\")+\"\\n\"\n",
    "s_clean_lines = s_clean_text.split(\"\\n\")[:-1]\n",
    "len(s_clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2274"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We remove empty spaces with split() and exclude empty lines  \n",
    "s_lines = [line for line in s_lines if len(line.strip()) > 0]\n",
    "len(s_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2271"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_clean_lines = [line for line in s_clean_lines if len(line.strip()) > 0]\n",
    "len(s_clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of lines of s_lines and s_clean_lines must be the same for this evaluation\n",
    "# Because in this case OCR incorrectly recognised lines will not be taken into account\n",
    "# Therefore we will save concatinated text without empty lines in two new files, which I will manually edit and upload here\n",
    "# I will replace missing lines with something (in this case \".\")\n",
    "\n",
    "s_text = \"\\n\".join(s_lines)\n",
    "s_clean_text = \"\\n\".join(s_clean_lines)\n",
    "\n",
    "with open('./txt/slovo_text_must_replace_missing_lines.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(s_text)\n",
    "with open('./txt/slovo_clean_text_must_replace_missing_lines.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(s_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daugava_clean_with_matching_lines..txt\n",
      "daugava_with_matching_lines..txt\n",
      "slovo_clean_with_matching_lines..txt\n",
      "slovo_with_matching_lines.txt\n",
      "\n",
      "2272\n",
      "2272\n"
     ]
    }
   ],
   "source": [
    "new_files = [f for f in Path(\"./\").glob(\"*.txt\")]\n",
    "[print(f) for f in new_files]\n",
    "print()\n",
    "\n",
    "with open(new_files[-1], encoding=\"utf-8\") as f:\n",
    "    s_lines = f.readlines()\n",
    "print(len(s_lines))\n",
    "\n",
    "with open(new_files[-2], encoding=\"utf-8\") as f:\n",
    "    s_clean_lines = f.readlines()\n",
    "print(len(s_clean_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate 0.18634354019977076\n",
      "Characters Error Rate 0.044660047566914565\n",
      "Word Information Lost 0.3331552163919098\n",
      "Match Error Rate 0.18565951545803083\n"
     ]
    }
   ],
   "source": [
    "# now we are ready to compare truth and original\n",
    "s_wer = jiwer.wer(s_clean_lines, s_lines)\n",
    "s_cer = jiwer.cer(s_clean_lines, s_lines)\n",
    "s_wil = jiwer.wil(s_clean_lines, s_lines)\n",
    "s_mer = jiwer.mer(s_clean_lines, s_lines)\n",
    "\n",
    "print(\"Word Error Rate\", s_wer)  # so 0.19 would mean 19% of words were wrong\n",
    "print(\"Characters Error Rate\", s_cer) # so 4% of all characters were wrong,missing or duplicates)\n",
    "print(\"Word Information Lost\", s_wil)\n",
    "print(\"Match Error Rate\", s_mer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↓ DAUGAVA ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('txt/Daugava/dauh1989n001_072.txt'),\n",
       " WindowsPath('txt/Daugava/dauh1989n009_089.txt'),\n",
       " WindowsPath('txt/Daugava/dauh1990n001_072.txt'),\n",
       " WindowsPath('txt/Daugava/dauh1990n010_094.txt'),\n",
       " WindowsPath('txt/Daugava/dauh1994n007-008_143.txt')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daugava = [f for f in Path(\"./txt/Daugava\").glob(\"*.txt\")]\n",
    "daugava = sorted(daugava, key=lambda elem: elem.name)\n",
    "daugava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('txt/Daugava..._clean/dauh1989n001_072_clean.txt'),\n",
       " WindowsPath('txt/Daugava..._clean/dauh1989n009_089_clean.txt'),\n",
       " WindowsPath('txt/Daugava..._clean/dauh1990n001_072_clean.txt'),\n",
       " WindowsPath('txt/Daugava..._clean/dauh1990n010_094_clean.txt'),\n",
       " WindowsPath('txt/Daugava..._clean/dauh1994n007-008_143_clean.txt')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daugava_clean  = [f for f in Path(\"txt/Daugava..._clean\").glob(\"*.txt\")]\n",
    "daugava_clean = sorted(daugava_clean, key=lambda elem: elem.name)\n",
    "daugava_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_text = \"\"\n",
    "for f in daugava:\n",
    "    d_text += f.read_text(encoding=\"utf-8\")+\"\\n\"\n",
    "d_lines = d_text.split(\"\\n\")[:-1]\n",
    "len(d_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_clean_text = \"\"\n",
    "for f in daugava_clean:\n",
    "    d_clean_text += f.read_text(encoding=\"utf-8\")+\"\\n\"\n",
    "d_clean_lines = d_clean_text.split(\"\\n\")[:-1]\n",
    "len(d_clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_lines = [line for line in d_lines if len(line.strip()) > 0]\n",
    "len(d_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_clean_lines = [line for line in d_clean_lines if len(line.strip()) > 0]\n",
    "len(d_clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_text = \"\\n\".join(d_lines)\n",
    "d_clean_text = \"\\n\".join(d_clean_lines)\n",
    "\n",
    "with open('./txt/daugava_text_must_replace_missing_lines.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(d_text)\n",
    "with open('./txt/daugava_clean_text_must_replace_missing_lines.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(d_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt\\daugava_clean_text.txt\n",
      "txt\\daugava_text.txt\n",
      "txt\\slovo_clean_text.txt\n",
      "txt\\slovo_text.txt\n",
      "\n",
      "420\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "new_files = [f for f in Path(\"txt\").glob(\"*.txt\")]\n",
    "[print(f) for f in new_files]\n",
    "print()\n",
    "\n",
    "with open(new_files[1], encoding=\"utf-8\") as f:\n",
    "    d_lines = f.readlines()\n",
    "print(len(d_lines))\n",
    "\n",
    "with open(new_files[0], encoding=\"utf-8\") as f:\n",
    "    d_clean_lines = f.readlines()\n",
    "print(len(d_clean_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate 0.031141868512110725\n",
      "Characters Error Rate 0.010582799224921747\n",
      "Word Information Lost 0.042355637894460396\n",
      "Match Error Rate 0.03111111111111111\n"
     ]
    }
   ],
   "source": [
    "d_wer = jiwer.wer(d_clean_lines, d_lines)\n",
    "d_cer = jiwer.cer(d_clean_lines, d_lines)\n",
    "d_wil = jiwer.wil(d_clean_lines, d_lines)\n",
    "d_mer = jiwer.mer(d_clean_lines, d_lines)\n",
    "\n",
    "print(\"Word Error Rate\", d_wer)  # so 0.03 would mean 3% of words were wrong\n",
    "print(\"Characters Error Rate\", d_cer) # so 1% of all characters were wrong,missing or duplicates)\n",
    "print(\"Word Information Lost\", d_wil)\n",
    "print(\"Match Error Rate\", d_mer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t Slovo... \t Daugava...\n",
      "Word Error Rate\t\t 0.18634 \t 0.03114\n",
      "Characters Error Rate\t 0.04466 \t 0.01058\n",
      "Word Information Lost\t 0.33316 \t 0.04236\n",
      "Match Error Rate\t 0.18566 \t 0.03111\n"
     ]
    }
   ],
   "source": [
    "# Conclusion\n",
    "print(\"\\t\\t\\t Slovo... \\t Daugava...\")\n",
    "print(\"Word Error Rate\\t\\t\", round(s_wer, 5), \"\\t\", round(d_wer, 5))\n",
    "print(\"Characters Error Rate\\t\", round(s_cer, 5), \"\\t\", round(d_cer, 5))\n",
    "print(\"Word Information Lost\\t\", round(s_wil, 5), \"\\t\", round(d_wil, 5))\n",
    "print(\"Match Error Rate\\t\", round(s_mer, 5), \"\\t\", round(d_mer, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
